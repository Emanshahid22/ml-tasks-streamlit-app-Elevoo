{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1bfce1bd",
      "metadata": {
        "id": "1bfce1bd"
      },
      "source": [
        "\n",
        "# 📊 Machine Learning Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d549c4be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d549c4be",
        "outputId": "4252862f-b8c0-46c9-92b0-fa8dca8e8154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ml_full_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ml_full_app.py\n",
        "# ml_full_app.py\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "st.set_page_config(page_title=\"ML Tasks:\", layout=\"wide\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io, zipfile, os, tempfile, shutil\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Optional imports - only used if available / installed\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    SMOTE = None\n",
        "\n",
        "try:\n",
        "    import librosa\n",
        "    import librosa.display\n",
        "except Exception:\n",
        "    librosa = None\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, applications\n",
        "except Exception:\n",
        "    tf = None\n",
        "\n",
        "# Utility functions\n",
        "def show_dataframe(df):\n",
        "    st.write(f\"Shape: {df.shape}\")\n",
        "    st.dataframe(df.head(50))\n",
        "\n",
        "def basic_info(df):\n",
        "    st.write(\"### Basic Info\")\n",
        "    st.write(df.describe(include='all').T)\n",
        "\n",
        "def plot_correlation(df):\n",
        "    st.write(\"### Correlation heatmap\")\n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "    sns.heatmap(df.select_dtypes(include=np.number).corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "def safe_read_csv(uploaded_file):\n",
        "    try:\n",
        "        return pd.read_csv(uploaded_file)\n",
        "    except Exception:\n",
        "        uploaded_file.seek(0)\n",
        "        return pd.read_csv(uploaded_file, encoding='latin1')\n",
        "\n",
        "st.sidebar.title(\"Tasks\")\n",
        "task = st.sidebar.radio(\"Choose a task:\", [\n",
        "    \"Task 1 — Student Score Prediction (Regression)\",\n",
        "    \"Task 2 — Customer Segmentation (Clustering)\",\n",
        "    \"Task 3 — Forest Cover Type (Multi-class Classification)\",\n",
        "    \"Task 4 — Loan Approval (Binary Classification, Imbalanced)\",\n",
        "    \"Task 5 — Movie Recommendation (Collaborative Filtering)\",\n",
        "    \"Task 6 — Music Genre Classification (Audio features)\",\n",
        "    \"Task 7 — Sales Forecasting (Time Series / Regression)\",\n",
        "    \"Task 8 — Traffic Sign Recognition (Image Classification)\"\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# TASK 1: Student Score Prediction (Regression)\n",
        "# -------------------------\n",
        "if task.startswith(\"Task 1\"):\n",
        "    st.header(\"Task 1 — Student Score Prediction (Regression)\")\n",
        "    st.markdown(\"Upload a CSV with student features (e.g. 'study_hours', 'sleep', 'participation') and the target column (e.g. 'score').\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload CSV for Task 1\", type=[\"csv\"])\n",
        "    if uploaded:\n",
        "        df = safe_read_csv(uploaded)\n",
        "        show_dataframe(df)\n",
        "        basic_info(df)\n",
        "        plot_correlation(df)\n",
        "\n",
        "        target = st.selectbox(\"Select target column (regression)\", options=df.columns.tolist())\n",
        "        features = st.multiselect(\"Select features\", [c for c in df.columns if c != target])\n",
        "\n",
        "        poly = st.checkbox(\"Try polynomial features (degree 2)\")\n",
        "        test_size = st.slider(\"Test set size (fraction)\", 0.05, 0.5, 0.2)\n",
        "\n",
        "        if st.button(\"Train Regression Model\"):\n",
        "            X = df[features].copy()\n",
        "            y = df[target].copy()\n",
        "            # simple numeric impute\n",
        "            X = X.select_dtypes(include=[np.number])\n",
        "            imputer = SimpleImputer(strategy='mean')\n",
        "            X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "            if poly:\n",
        "                pf = PolynomialFeatures(degree=2, include_bias=False)\n",
        "                X_train = pf.fit_transform(X_train)\n",
        "                X_test = pf.transform(X_test)\n",
        "\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            st.write(\"MSE:\", mean_squared_error(y_test, preds))\n",
        "            st.write(\"R²:\", r2_score(y_test, preds))\n",
        "\n",
        "            # plot\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.scatter(y_test, preds, alpha=0.6)\n",
        "            ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "            ax.set_xlabel(\"Actual\")\n",
        "            ax.set_ylabel(\"Predicted\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "\n",
        "            from sklearn.metrics import silhouette_score\n",
        "\n",
        "if st.checkbox(\"Run Elbow Method\"):\n",
        "    inertias = []\n",
        "    sils = []\n",
        "    K_range = range(2, 11)\n",
        "    for k in K_range:\n",
        "        km = KMeans(n_clusters=k, random_state=42).fit(Xs)\n",
        "        inertias.append(km.inertia_)\n",
        "        sils.append(silhouette_score(Xs, km.labels_))\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax1.plot(K_range, inertias, 'bo-', label='Inertia')\n",
        "    ax1.set_xlabel(\"k\")\n",
        "    ax1.set_ylabel(\"Inertia\")\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(K_range, sils, 'ro-', label='Silhouette')\n",
        "    ax2.set_ylabel(\"Silhouette\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "\n",
        "    if 'kmeans_cluster' in df.columns:\n",
        "      st.write(\"Average feature values per cluster:\")\n",
        "      st.write(df.groupby('kmeans_cluster')[features].mean())\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# TASK 2: Customer Segmentation (Clustering)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 2\"):\n",
        "    st.header(\"Task 2 — Customer Segmentation (Clustering)\")\n",
        "    st.markdown(\"Upload a CSV (e.g., Mall Customers dataset). Choose numeric features for clustering. You can run KMeans and DBSCAN.\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload CSV for Task 2\", type=[\"csv\"])\n",
        "    if uploaded:\n",
        "        df = safe_read_csv(uploaded)\n",
        "        show_dataframe(df)\n",
        "\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        st.write(\"Numeric columns detected:\", numeric_cols)\n",
        "        features = st.multiselect(\"Select numeric features for clustering\", numeric_cols, default=numeric_cols[:2])\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        if features:\n",
        "            X = df[features].copy().fillna(df[features].median())\n",
        "            Xs = scaler.fit_transform(X)\n",
        "\n",
        "            run_kmeans = st.checkbox(\"Run KMeans\")\n",
        "            run_dbscan = st.checkbox(\"Run DBSCAN\")\n",
        "            if run_kmeans:\n",
        "                k = st.slider(\"K for KMeans\", 2, 12, 3)\n",
        "                km = KMeans(n_clusters=k, random_state=42)\n",
        "                labels = km.fit_predict(Xs)\n",
        "                df['kmeans_cluster'] = labels\n",
        "                st.write(\"Cluster counts:\")\n",
        "                st.write(df['kmeans_cluster'].value_counts())\n",
        "                if len(features) >= 2:\n",
        "                    fig, ax = plt.subplots()\n",
        "                    ax.scatter(X.iloc[:,0], X.iloc[:,1], c=labels, cmap='tab10', alpha=0.7)\n",
        "                    ax.set_xlabel(features[0]); ax.set_ylabel(features[1])\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "            if run_dbscan:\n",
        "                eps = st.slider(\"DBSCAN eps\", 0.1, 5.0, 0.9)\n",
        "                min_samples = st.slider(\"DBSCAN min_samples\", 2, 20, 5)\n",
        "                db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "                labels = db.fit_predict(Xs)\n",
        "                df['dbscan_cluster'] = labels\n",
        "                st.write(\"DBSCAN labels ( -1 = noise ) value counts:\")\n",
        "                st.write(df['dbscan_cluster'].value_counts())\n",
        "                if len(features) >= 2:\n",
        "                    fig, ax = plt.subplots()\n",
        "                    ax.scatter(X.iloc[:,0], X.iloc[:,1], c=labels, cmap='tab10', alpha=0.7)\n",
        "                    ax.set_xlabel(features[0]); ax.set_ylabel(features[1])\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "# -------------------------\n",
        "# TASK 3: Forest Cover Type Classification (Multi-class)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 3\"):\n",
        "    st.header(\"Task 3 — Forest Cover Type Classification (Multi-class)\")\n",
        "    st.markdown(\"Upload a CSV. Choose features and the target column (multi-class). We'll train RandomForest and optionally XGBoost.\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload CSV for Task 3\", type=[\"csv\"])\n",
        "    if uploaded:\n",
        "        df = safe_read_csv(uploaded)\n",
        "        show_dataframe(df)\n",
        "        basic_info(df)\n",
        "\n",
        "        target = st.selectbox(\"Select target column\", options=df.columns.tolist())\n",
        "        features = st.multiselect(\"Select features\", [c for c in df.columns if c != target], default=[c for c in df.columns if c != target][:8])\n",
        "        test_size = st.slider(\"Test set size\", 0.05, 0.5, 0.2)\n",
        "\n",
        "        if st.button(\"Train Classifier\"):\n",
        "            X = df[features].copy()\n",
        "            y = df[target].copy()\n",
        "            X = pd.get_dummies(X, drop_first=True)\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            rf.fit(X_train, y_train)\n",
        "            preds = rf.predict(X_test)\n",
        "            st.write(\"RandomForest classification report:\")\n",
        "            st.text(classification_report(y_test, preds))\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(6,5))\n",
        "            sns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt='d', ax=ax)\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            if xgb is not None:\n",
        "                st.write(\"Training XGBoost (if installed)...\")\n",
        "                xclf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "                xclf.fit(X_train, y_train)\n",
        "                xp = xclf.predict(X_test)\n",
        "                st.write(\"XGBoost classification report:\")\n",
        "                st.text(classification_report(y_test, xp))\n",
        "            else:\n",
        "                st.info(\"XGBoost not installed — skip.\")\n",
        "\n",
        "# -------------------------\n",
        "# TASK 4: Loan Approval Prediction (Binary classification, imbalanced)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 4\"):\n",
        "    st.header(\"Task 4 — Loan Approval Prediction (Binary Classification)\")\n",
        "    st.markdown(\"Upload a CSV. We will impute, encode categorical variables, optionally apply SMOTE, and train Logistic Regression / RandomForest.\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload CSV for Task 4\", type=[\"csv\"])\n",
        "    if uploaded:\n",
        "        df = safe_read_csv(uploaded)\n",
        "        show_dataframe(df)\n",
        "        basic_info(df)\n",
        "\n",
        "        target = st.selectbox(\"Select binary target column\", options=df.columns.tolist())\n",
        "        features = st.multiselect(\"Select features\", [c for c in df.columns if c != target], default=[c for c in df.columns if c != target][:8])\n",
        "        apply_smote = st.checkbox(\"Apply SMOTE (if imbalanced and imblearn installed)\")\n",
        "        test_size = st.slider(\"Test size\", 0.05, 0.5, 0.2)\n",
        "\n",
        "        if st.button(\"Train Loan Approval Model\"):\n",
        "            X = df[features].copy()\n",
        "            y = df[target].copy()\n",
        "            # Encode categoricals\n",
        "            X = pd.get_dummies(X, drop_first=True)\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "            # Label-encode y if needed\n",
        "            if y.dtype == 'O' or y.dtype.name == 'category':\n",
        "                le = LabelEncoder()\n",
        "                y = le.fit_transform(y)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y if len(np.unique(y))>1 else None, random_state=42)\n",
        "\n",
        "            if apply_smote and SMOTE is not None:\n",
        "                sm = SMOTE(random_state=42)\n",
        "                X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "            elif apply_smote:\n",
        "                st.info(\"imblearn not installed; SMOTE skipped.\")\n",
        "\n",
        "            lr = LogisticRegression(max_iter=1000)\n",
        "            lr.fit(X_train, y_train)\n",
        "            preds = lr.predict(X_test)\n",
        "            st.write(\"Logistic Regression report:\")\n",
        "            st.text(classification_report(y_test, preds))\n",
        "            fig, ax = plt.subplots()\n",
        "            sns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt='d', ax=ax)\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            rf.fit(X_train, y_train)\n",
        "            preds_rf = rf.predict(X_test)\n",
        "            st.write(\"RandomForest report:\")\n",
        "            st.text(classification_report(y_test, preds_rf))\n",
        "\n",
        "# -------------------------\n",
        "# TASK 5: Movie Recommendation (Collaborative Filtering)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 5\"):\n",
        "    st.header(\"Task 5 — Movie Recommendation System (User-based CF)\")\n",
        "    st.markdown(\"Upload a ratings CSV with columns: userId, movieId, rating (or adapt columns via selection).\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload CSV for Task 5\", type=[\"csv\"])\n",
        "    if uploaded:\n",
        "        df = safe_read_csv(uploaded)\n",
        "        show_dataframe(df)\n",
        "        cols = df.columns.tolist()\n",
        "        ucol = st.selectbox(\"User ID column\", cols, index=0)\n",
        "        mcol = st.selectbox(\"Item (movie) ID column\", cols, index=1)\n",
        "        rcol = st.selectbox(\"Rating column\", cols, index=2)\n",
        "        n_neighbors = st.slider(\"Number of similar users to use\", 1, 50, 5)\n",
        "\n",
        "        if st.button(\"Build and Recommend\"):\n",
        "            pivot = df.pivot_table(index=ucol, columns=mcol, values=rcol).fillna(0)\n",
        "            st.write(\"User-Item matrix shape:\", pivot.shape)\n",
        "            # use cosine similarity\n",
        "            sim = cosine_similarity(pivot)\n",
        "            sim_df = pd.DataFrame(sim, index=pivot.index, columns=pivot.index)\n",
        "\n",
        "            selected_user = st.selectbox(\"Choose a user to get recommendations for\", pivot.index.tolist())\n",
        "            user_sim = sim_df[selected_user].sort_values(ascending=False).drop(selected_user).head(n_neighbors)\n",
        "            st.write(\"Top similar users:\", user_sim.head())\n",
        "\n",
        "            # aggregate ratings from top similar users\n",
        "            similar_users = user_sim.index\n",
        "            weighted_scores = pivot.loc[similar_users].T.dot(user_sim)\n",
        "            recommended = weighted_scores.sort_values(ascending=False).index.difference(pivot.loc[selected_user][pivot.loc[selected_user]>0].index)\n",
        "            st.write(\"Top recommended item IDs (movieId):\")\n",
        "            st.write(recommended[:20].tolist())\n",
        "\n",
        "# -------------------------\n",
        "# TASK 6: Music Genre Classification (Audio features)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 6\"):\n",
        "    st.header(\"Task 6 — Music Genre Classification (Audio features)\")\n",
        "    st.markdown(\"Two options:\\n1) Upload a CSV with precomputed audio features (MFCCs, chroma, etc.)\\n2) Upload a ZIP of audio files (wav, mp3) and the app will extract MFCCs (requires librosa).\")\n",
        "\n",
        "    choice_opt = st.radio(\"Input type\", [\"Features CSV\", \"ZIP of audio files + labels.csv\"])\n",
        "    if choice_opt == \"Features CSV\":\n",
        "        uploaded = st.file_uploader(\"Upload features CSV\", type=[\"csv\"])\n",
        "        if uploaded:\n",
        "            df = safe_read_csv(uploaded)\n",
        "            show_dataframe(df)\n",
        "            target = st.selectbox(\"Select target column\", df.columns.tolist())\n",
        "            features = st.multiselect(\"Select feature columns\", [c for c in df.columns if c != target], default=[c for c in df.columns if c!=target][:20])\n",
        "            test_size = st.slider(\"Test size\", 0.05, 0.4, 0.2)\n",
        "            if st.button(\"Train classifier on features\"):\n",
        "                X = df[features].fillna(0)\n",
        "                y = df[target]\n",
        "                le = LabelEncoder()\n",
        "                y = le.fit_transform(y)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "                rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                rf.fit(X_train, y_train)\n",
        "                preds = rf.predict(X_test)\n",
        "                st.write(classification_report(y_test, preds))\n",
        "    else:\n",
        "        uploaded_zip = st.file_uploader(\"Upload ZIP (audio files + labels.csv where labels.csv has columns filename, genre)\", type=[\"zip\"])\n",
        "        if uploaded_zip:\n",
        "            if librosa is None:\n",
        "                st.error(\"librosa not installed in this environment. Install librosa to extract audio features.\")\n",
        "            else:\n",
        "                tfolder = tempfile.mkdtemp()\n",
        "                with zipfile.ZipFile(uploaded_zip) as z:\n",
        "                    z.extractall(tfolder)\n",
        "                labels_path = os.path.join(tfolder, \"labels.csv\")\n",
        "                if not os.path.exists(labels_path):\n",
        "                    st.error(\"labels.csv not found inside the ZIP. Create labels.csv with columns: filename, genre\")\n",
        "                else:\n",
        "                    labels_df = pd.read_csv(labels_path)\n",
        "                    feats = []\n",
        "                    y = []\n",
        "                    st.info(\"Extracting MFCCs (this can take some time)...\")\n",
        "                    for idx, row in labels_df.iterrows():\n",
        "                        fpath = os.path.join(tfolder, row['filename'])\n",
        "                        if not os.path.exists(fpath):\n",
        "                            continue\n",
        "                        try:\n",
        "                            y_samp, sr = librosa.load(fpath, sr=22050, mono=True)\n",
        "                            mfcc = librosa.feature.mfcc(y=y_samp, sr=sr, n_mfcc=20)\n",
        "                            mfcc_mean = np.mean(mfcc, axis=1)\n",
        "                            feats.append(mfcc_mean)\n",
        "                            y.append(row['genre'])\n",
        "                        except Exception as e:\n",
        "                            st.write(\"Error processing\", fpath, e)\n",
        "                    if feats:\n",
        "                        X = np.vstack(feats)\n",
        "                        le = LabelEncoder()\n",
        "                        y_enc = le.fit_transform(y)\n",
        "                        X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.2, random_state=42)\n",
        "                        rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "                        rf.fit(X_train, y_train)\n",
        "                        preds = rf.predict(X_test)\n",
        "                        st.write(classification_report(y_test, preds, target_names=le.classes_))\n",
        "                shutil.rmtree(tfolder)\n",
        "\n",
        "# -------------------------\n",
        "# TASK 7: Sales Forecasting (Time series)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 7\"):\n",
        "    st.header(\"Task 7 — Sales Forecasting (Time Series)\")\n",
        "    st.markdown(\"Upload a CSV with at least a date column and a sales column. We'll create simple lag features and train a regression model.\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload CSV for Task 7\", type=[\"csv\"])\n",
        "    if uploaded:\n",
        "        df = safe_read_csv(uploaded)\n",
        "        show_dataframe(df)\n",
        "        date_col = st.selectbox(\"Date column\", df.columns.tolist())\n",
        "        sales_col = st.selectbox(\"Sales column\", df.columns.tolist())\n",
        "        n_lags = st.slider(\"Number of lag features to create\", 1, 24, 3)\n",
        "\n",
        "        if st.button(\"Build forecasting model\"):\n",
        "            df[date_col] = pd.to_datetime(df[date_col])\n",
        "            df = df.sort_values(date_col)\n",
        "            df = df.set_index(date_col)\n",
        "            series = df[sales_col].astype(float).fillna(method='ffill')\n",
        "            feat_df = pd.DataFrame({ 'y': series })\n",
        "            for lag in range(1, n_lags+1):\n",
        "                feat_df[f'lag_{lag}'] = feat_df['y'].shift(lag)\n",
        "            feat_df['month'] = feat_df.index.month\n",
        "            feat_df = feat_df.dropna()\n",
        "            X = feat_df.drop(columns=['y'])\n",
        "            y = feat_df['y']\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "            model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "            st.write(\"MSE:\", mean_squared_error(y_test, preds))\n",
        "            st.write(\"R²:\", r2_score(y_test, preds))\n",
        "            fig, ax = plt.subplots(figsize=(10,4))\n",
        "            ax.plot(y_test.index, y_test.values, label='Actual')\n",
        "            ax.plot(y_test.index, preds, label='Predicted')\n",
        "            ax.legend()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "# -------------------------\n",
        "# TASK 8: Traffic Sign Recognition (Image classification)\n",
        "# -------------------------\n",
        "elif task.startswith(\"Task 8\"):\n",
        "    st.header(\"Task 8 — Traffic Sign Recognition (Image Classification)\")\n",
        "    st.markdown(\"Upload a ZIP containing image folders or images with a labels.csv. For quick runs you can upload a small dataset and use transfer learning (requires TensorFlow).\")\n",
        "\n",
        "    uploaded = st.file_uploader(\"Upload ZIP for Task 8 (images + labels.csv or folders)\", type=[\"zip\"])\n",
        "    if uploaded:\n",
        "        if tf is None:\n",
        "            st.error(\"TensorFlow not installed. Install tensorflow to run image classification training.\")\n",
        "        else:\n",
        "            tmpdir = tempfile.mkdtemp()\n",
        "            with zipfile.ZipFile(uploaded) as z:\n",
        "                z.extractall(tmpdir)\n",
        "\n",
        "            # Option A: labels.csv exists\n",
        "            labels_csv = os.path.join(tmpdir, \"labels.csv\")\n",
        "            image_paths = []\n",
        "            labels = []\n",
        "            if os.path.exists(labels_csv):\n",
        "                labdf = pd.read_csv(labels_csv)\n",
        "                for _, r in labdf.iterrows():\n",
        "                    f = os.path.join(tmpdir, r['filename'])\n",
        "                    if os.path.exists(f):\n",
        "                        image_paths.append(f)\n",
        "                        labels.append(r['label'])\n",
        "            else:\n",
        "                # Option B: folder per class\n",
        "                for root, dirs, files in os.walk(tmpdir):\n",
        "                    for d in dirs:\n",
        "                        dpath = os.path.join(root, d)\n",
        "                        for fname in os.listdir(dpath):\n",
        "                            if fname.lower().endswith(('.png','.jpg','.jpeg')):\n",
        "                                image_paths.append(os.path.join(dpath, fname))\n",
        "                                labels.append(d)\n",
        "\n",
        "            if not image_paths:\n",
        "                st.error(\"No images found or labels.csv missing/incorrect.\")\n",
        "            else:\n",
        "                st.write(f\"Found {len(image_paths)} images across {len(set(labels))} classes.\")\n",
        "                # Small dataset loader (resize)\n",
        "                img_size = st.slider(\"Image size (px)\", 64, 224, 128)\n",
        "                def load_and_preprocess(paths, labels):\n",
        "                    X = []\n",
        "                    y = []\n",
        "                    for p, lab in zip(paths, labels):\n",
        "                        img = tf.keras.preprocessing.image.load_img(p, target_size=(img_size, img_size))\n",
        "                        arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "                        X.append(arr)\n",
        "                        y.append(lab)\n",
        "                    return np.array(X), np.array(y)\n",
        "                X, y = load_and_preprocess(image_paths, labels)\n",
        "                le = LabelEncoder(); y_enc = le.fit_transform(y)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.2, random_state=42, stratify=y_enc)\n",
        "                st.write(\"Training shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "                if st.button(\"Train Transfer-learning (MobileNetV2)\"):\n",
        "                    base = applications.MobileNetV2(include_top=False, input_shape=(img_size, img_size, 3), pooling='avg', weights='imagenet')\n",
        "                    base.trainable = False\n",
        "                    model = models.Sequential([base, layers.Dense(128, activation='relu'), layers.Dropout(0.3), layers.Dense(len(le.classes_), activation='softmax')])\n",
        "                    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "                    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
        "                    loss, acc = model.evaluate(X_test, y_test)\n",
        "                    st.write(\"Test accuracy:\", acc)\n",
        "                    preds = model.predict(X_test).argmax(axis=1)\n",
        "                    st.write(classification_report(y_test, preds, target_names=le.classes_))\n",
        "            shutil.rmtree(tmpdir)\n",
        "\n",
        "# Footer\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.write(\"This app is a compact, educational dashboard that demonstrates standard ML workflows for a variety of tasks.\")\n",
        "st.write(\"Made for learning and quick prototyping. For production, add robust logging, saving/loading models, monitoring, and error handling.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85565b6",
      "metadata": {
        "id": "d85565b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fa784c-faf8-4e91-ded0-5a0c0cd1e37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q streamlit pyngrok seaborn scikit-learn pandas numpy matplotlib\n",
        "# Optional extras (for advanced tasks)\n",
        "!pip install -q xgboost imbalanced-learn librosa tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89f449b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c89f449b",
        "outputId": "b1e7351b-ba10-4ff5-f162-28ef1100c6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Replace with your token (from https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "!ngrok config add-authtoken 33KvSSpeeM4QsTbnxHMJobyrDOM_828VvJQ9C33d3SA1k1jv7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d620baa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d620baa",
        "outputId": "5068dfa2-264b-4876-90fe-e7f4e8d1b30a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://offenseless-margurite-embryologic.ngrok-free.dev\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start streamlit in background\n",
        "get_ipython().system_raw(\"streamlit run ml_full_app.py --server.port 8501 &\")\n",
        "\n",
        "# Open tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}